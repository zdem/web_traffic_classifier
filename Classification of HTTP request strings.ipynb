{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of HTTP request strings\n",
    "\n",
    "## Task\n",
    "Given the [CSIC 2010 dataset](http://www.isi.csic.es/dataset/) containing HTTP requests labelled as 'normal' and 'anomalous' build a classifier able to distinguish between normal and anomalous (potentially malicious) requests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach\n",
    "A brief literature survey reveals many approaches that have been used for this task. Our preferred approach is based on the recent work of [Althubiti et al](https://digitalcommons.kennesaw.edu/ccerp/2017/practice/2?utm_source=digitalcommons.kennesaw.edu%2Fccerp%2F2017%2Fpractice%2F2&utm_medium=PDF&utm_campaign=PDFCoverPages) from last year which shows that a simple logistic regression built on extracting five features from each HTTP request should give excellent results. Those five features are:\n",
    "1. Length of the request\n",
    "2. Length of the arguments\n",
    "3. Number of arguments\n",
    "4. Length of the path\n",
    "5. Number of 'special' chars in the path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing of the input data\n",
    "Combine the two data sets labelled as 'normal' into the file 'input_data/normalTrafficAll.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries needed for the parsing step\n",
    "import re\n",
    "import numpy as np\n",
    "from urlparse import urlparse, parse_qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['input_data/normalTrafficTest.txt','input_data/normalTrafficTraining.txt']\n",
    "with open('input_data/normalTrafficAll.txt', 'w') as outfile:\n",
    "    for input_file in files:\n",
    "        with open(input_file) as infile:\n",
    "            for line in infile:\n",
    "                outfile.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the individual HTTP requests can be done splitting the contents of the input file on the strings 'GET ', 'POST ', 'PUT ' which mark the beginning of each request. We have implemented a class called `dataset` which can perform parsing of any file from the CSIC 2010 data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset:\n",
    "\n",
    "        # Read-in the dataset and parse it into individual HTTP requests.\n",
    "        def __init__(self, path_to_file):\n",
    "\n",
    "                self.HTTP_requests = []\n",
    "                self.n_requests = 0\n",
    "                self.path_to_file = path_to_file\n",
    "\n",
    "                with open(path_to_file, 'r') as input_file: data = input_file.read()\n",
    "\n",
    "                # Split the raw data into individual methods (GET, POST, PUT) request strings\n",
    "                methods_requests = re.split('(GET |POST |PUT )', data)\n",
    "                methods_requests.pop(0)\n",
    "\n",
    "                only_methods = methods_requests[::2]\n",
    "                only_requests = methods_requests[1:][::2]\n",
    "\n",
    "                self.HTTP_requests =  [method + request for method,request in zip(only_methods,only_requests)]\n",
    "\n",
    "                self.n_requests = len(self.HTTP_requests)\n",
    "                print \"\\nFound \", self.n_requests, \" HTTP requests in file: \", path_to_file\n",
    "\n",
    "        # Generate the HTTP features for all HTTP requests and label them.\n",
    "        def extract_labelled_HTTP_features(self,label):\n",
    "\n",
    "                X = []\n",
    "                y = []\n",
    "\n",
    "                for HTTP_request in self.HTTP_requests:\n",
    "                        X.append(extract_features(HTTP_request))\n",
    "                        y.append(label)\n",
    "\n",
    "                print \"\\nData from\",self.path_to_file,\"have been exported\"\n",
    "                print \"Label assigned:\", label\n",
    "\n",
    "                return X, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing of the input file is performed by the constructor `__init__`. The method `extract_labelled_HTTP_features` can be applied to extract the features for all HTTP requests in the data set. This method uses the function `extract_features` which extracts from each HTTP request the five features listed above. This function looks as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the five features from a single HTTP_request as described in Althubiti et al paper.\n",
    "# These are formed from the URI only: the rest of the HTTP request is not useful.\n",
    "def extract_features(HTTP_request, debug = False):\n",
    "        features = np.zeros(5)\n",
    "\n",
    "        # Extract the method and the URI\n",
    "        first_line = re.match('(.*) (http.*) HTTP',HTTP_request)\n",
    "        method = first_line.group(1)\n",
    "        uri_string = first_line.group(2)\n",
    "\n",
    "        # Remove the redundant information from the request body\n",
    "        stripped_request = re.sub('User-Agent:.*Connection: close|[\\n\\r\\X]','',HTTP_request)\n",
    "\n",
    "        if (method != 'GET'):\n",
    "                post_query = re.search('Content-Length:\\s*\\d+(.*)',stripped_request)\n",
    "                arguments = re.sub('\\n|\\r','',post_query.group(1))\n",
    "                uri_string = uri_string + '?' + arguments\n",
    "\n",
    "        # Parse the URI into path and arguments\n",
    "        uri = urlparse(uri_string)\n",
    "\n",
    "        path = uri.path\n",
    "        arguments = parse_qs(uri.query)\n",
    "\n",
    "        #1. Length of the request\n",
    "        features[0] = len(uri_string)\n",
    "\n",
    "        #2. Length of the arguments\n",
    "        features[1] = 0\n",
    "        for parameter, value in arguments.iteritems():\n",
    "                features[1] += len(parameter+(''.join(value)))\n",
    "\n",
    "        #3. Number of arguments\n",
    "        features[2] = len(arguments)\n",
    "\n",
    "        #4. Length of the path\n",
    "        features[3] = len(path)\n",
    "\n",
    "        #5. Number of special chars in the path\n",
    "        features[4] = len(re.findall('\\W',path))\n",
    "\n",
    "        if (debug):\n",
    "                print \"\\n\" + method\n",
    "                print path\n",
    "                print arguments\n",
    "                print features\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `extract_features` only needs to use the part of the request containing the URL and the parameters. The rest of the HTTP request is discarded because it does not describe users's behavior. We proceed to build the full URI consiting of the URL and the parameters and parse them using the `urlparse` library. Finally, we construct the five features listed by Althubiti. Optionally, the results of the feature extraction can be printed to std output using the `debug` parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the class `dataset` to parse both the normal and the anomalous data sets. We have labelled the normal HTTP requests as 0 and the anomalous requests as 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_requests = dataset('input_data/normalTrafficAll.txt')\n",
    "X_normal, y_normal = normal_requests.extract_labelled_HTTP_features(0)\n",
    "\n",
    "anomalous_requests = dataset('input_data/anomalousTrafficTest.txt')\n",
    "X_anomalous, y_anomalous = anomalous_requests.extract_labelled_HTTP_features(1)\n",
    "\n",
    "X = np.concatenate((X_normal,X_anomalous),axis=0)\n",
    "y = np.concatenate((y_normal,y_anomalous),axis=0)\n",
    "\n",
    "print X\n",
    "print y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's illustrate the feature selection which just took place on a single HTTP request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print normal_requests.HTTP_requests[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_features(normal_requests.HTTP_requests[1],True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The printout shows all the values which have been used to construct features. Note that the parsing using `urlparse` library also takes care of capturing the Spanish Unicode characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting of the Logisitic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the `sklearn` library to perform the fitting:\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the whole data set into 60% for fitting and 40% for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.6, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Linear model using all training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = LogisticRegression(C=0.01)\n",
    "model1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model1.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The low value of recall for the anomalous class shows that this model is rather bad at finding all anomalous requests, i.e. the rate of false negatives is very high. On the other hand this test is good at identifying normal requests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Non-linear model using all training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 1 separates the two classes using a simple hyperplane. Let's see if we can improve the model by using a more compilcated non-linear boundary which uses all unique quadratic terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xquad_train = X_train\n",
    "Xquad_test = X_test\n",
    "col = np.zeros(len(y))\n",
    "for col_i in range(0,len(X_train[0,:])):\n",
    "        for col_j in range(0,col_i+1):\n",
    "                col = X_train[:,col_i] * X_train[:,col_j]\n",
    "                Xquad_train = np.column_stack((Xquad_train,col))\n",
    "\n",
    "                col_test = X_test[:,col_i] * X_test[:,col_j]\n",
    "                Xquad_test = np.column_stack((Xquad_test,col_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = LogisticRegression(C=0.01)\n",
    "model2.fit(Xquad_train, y_train)\n",
    "\n",
    "y_pred = model2.predict(Xquad_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, increasing the variability of the boundary improves all parameters of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Linear model using unique training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parsed data set contains pairs of equivalent GET and POST calls which are not distinguished by the features of the model. We can therefore train another model from data not containing the duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy = np.column_stack((X_train,y_train))\n",
    "Xy = np.asarray(np.unique(Xy, axis=0))\n",
    "\n",
    "n_samples = len(Xy)\n",
    "n_features = len(Xy[0])-1\n",
    "\n",
    "print \"Number of unique samples = \", n_samples\n",
    "print \"Number of features = \", n_features\n",
    "\n",
    "y_train = Xy[:,n_features]\n",
    "X_train = np.delete(Xy,n_features,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = LogisticRegression(C=0.01)\n",
    "model3.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model3.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of this model is bad for both classes. Clearly, the presence of the redundant data points puts more weight on the important parts of the hyperspace and results in a more accurate decision boundary. We could also construct a model including all unique quadratic features (this is done in the script `main.py`) but it does not improve the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "The performance of our models is significantly worse than the logistic regression models of Althubiti et al, see Table 3 ibid. The regularization parameter `C` in `LogisticRegression(C=0.01)` has only a minor effect on the final model. Possible causes of the underperformance of our model are:\n",
    "1. The regression solver is stuck in a local minimum: this seems unlikely since all the available solvers return very similar results. Scaling of the training data also doesn't improve the fitting.\n",
    "2. Althubiti et al perform some additonal cleaning of the training data which is not described in their paper.\n",
    "3. The use of a high-order decision boundary in the Althubiti paper.\n",
    "4. A mistake in our function `extract_features`: we have tried including also the features from Table 1 of Althubiti et al to see if the performance improves but the results were almost identical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "Our logistic regression models behave poorly in comparison with the one determined by Althubiti et al. The reason for that remains unknown. Our best model is the non-linear Model 2 including all data points which has recall of approx. 0.5 for the anomalous class. This means that it would correctly spot only about a half of all anomalous requests. As is the model is clearly deficient and should be fixed before it is applied in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical application\n",
    "Once the reason for the underperformance of our classifier wrt Althubiti one is found the classifier can be used in practice. Nevertheless, a blind application of this classifier as a hard rule may not be desirable depending on the purpose of the web application due to incidence of false positives. For example for an e-shop server the occurence of a false positive classification could mean that e.g. a particular expensive product with a long name would never be sold which is clearly an unacceptable behavior. This behavior is caused by the fact that the classifier is only statistical and therefore blind to the actual intent of the request.\n",
    "\n",
    "In order to reduce the incidence of false positives the HTTP request classification could be split into two stages:\n",
    "1. Pre-screening based on the statistical classifier. Requests labelled as normal are allowed.\n",
    "2. Further screening of the requests labelled as anomalous. Here we would apply a different classifier which would try to infer the intent of the request as belonging to a set of allowed operations, e.g. put a certain item in the shopping cart. This would require a non-statitical algorithm analyzing the type of parameters supplied in the request. With the intent inferred the web server could create its own HTTP request and compare it with the one supplied by the user. If they match the request would be allowed, otherwise rejected. This approach ensures that the request supplied by the user is always compared to a request which is free of any malicious code.\n",
    "\n",
    "### Function for classification of HTTP requests\n",
    "For simplicity we implement here the decision function `normal_or_anomalous` for Model 1 which therefore constitutes our predictor of normal vs anomalous HTTP requests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print model1.coef_\n",
    "print model1.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicts whether a given HTTP request string is normal (=0) or anomalous (=1)\n",
    "# The decision boundary has been determined by Model 1.\n",
    "def normal_or_anomalous(HTTP_request):\n",
    "\n",
    "        features = extract_features(HTTP_request)\n",
    "        theta = [0.15609435, -0.16087162, -0.23866712, -0.17492764, -0.0141425]\n",
    "\n",
    "        p = 1.0/(1+np.exp(-(np.dot(theta,features)-4.12389027)))\n",
    "        if p >= 0.5:\n",
    "                p = 1\n",
    "        else:\n",
    "                p = 0\n",
    "\n",
    "        return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function has been included in the stand-alone module `http_requests` and can be used to analyze individual HTTP requests in practice, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print normal_or_anomalous(normal_requests.HTTP_requests[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
